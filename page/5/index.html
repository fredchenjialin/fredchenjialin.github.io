<!DOCTYPE html><html lang="中文简体"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> MPZ Diary</title><meta name="description" content="使用Hexo搭建的一个Diary。"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon-32x32.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></head><body><div id="mask" style="display: none;"><img id="mask-image" src="#" style=" "></div><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon-96x96.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/fredchenjialin" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/fredchenjialin" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h1 class="post-title"><a href="/2015/11/12/最近没学习/" class="post-title-link">最近没学习</a></h1><div class="post-time">Nov 12, 2015</div><div class="post-content-excerpt"><p>这几天我去参加陕西省研究生创新成果展，看到了很多好玩的东西，也学习到了很多，有了收获，长了见识，认识教训。</p>
<p>所以我没有学习，而且也不太能学的下，不能够静心。</p>
<p>而且，我的手机也是今天也才到我的手上，也算是满足我了我的一个心愿吧，希望诸事安定，可以安心学习了。</p>
</div><a href="/2015/11/12/最近没学习/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h1 class="post-title"><a href="/2015/11/01/机器学习5/" class="post-title-link">机器学习5</a></h1><div class="post-time">Nov 1, 2015</div><div class="post-content-excerpt"><h3 id="第一节里面讲了一个大而全的针对神经网络的cost_Function">第一节里面讲了一个大而全的针对神经网络的cost Function</h3><p><img src="http://7nj2kj.com1.z0.glb.clouddn.com/ML-ml5/costfunction.png" alt="costfunction"></p>
<h3 id="Backpropagation_Algorithm">Backpropagation Algorithm</h3><ul>
<li><p>想要求导偏导数是比较困难的。以下是前向传播的算法：<br>  <img src="http://7nj2kj.com1.z0.glb.clouddn.com/ML-ml5/Forwardpropagation.png" alt="Forwardpropagation"><br>  可以看出会比较繁琐。</p>
</li>
<li><p>然后这是后向传播<br>  <img src="http://7nj2kj.com1.z0.glb.clouddn.com/ML-ml5/backpropagation.png" alt="backpropagation"><br>  <img src="http://7nj2kj.com1.z0.glb.clouddn.com/ML-ml5/backpropagationAch.png" alt="backpropagation"></p>
</li>
</ul>
<h3 id="NG说了back的这个也是相当的复杂的，所以，我再说一节给大家感受一下">NG说了back的这个也是相当的复杂的，所以，我再说一节给大家感受一下</h3><ul>
<li>说到了一个梯度检验算法，用来证明你做的梯度下降是没有错的。</li>
</ul>
<h2 id="程序作业">程序作业</h2><h3 id="第一项_理解多种结果的逻辑回归分类并计算cost_function">第一项 理解多种结果的逻辑回归分类并计算cost function</h3><h4 id="多种化为传统">多种化为传统</h4><p>在原来的看法中，以0，1为例，我们将产生这两种结果的cost function定义如下：</p>
<p><img src="http://7nj2kj.com1.z0.glb.clouddn.com/ML-ml3/2.png" alt="LinearOneVariable"></p>
<p>若是y有多种结果，我们可以假想为，若有n种结果</p>
<ol>
<li>第一个函数计算 （y == x）</li>
<li>第二个函数计算 （y != x）</li>
</ol>
<p>如此实现了从多种到传统的过度。</p>
<h4 id="函数整合">函数整合</h4><p>针对两个函数整合到一起的函数使用的是（y）和（1-y）的小技巧，实现了不是第一个函数就是第二个函数的功能。</p>
<h4 id="举例假设">举例假设</h4><ol>
<li>有5000个样例，10个分类，所以基本的特征函数产生有5000*10的矩阵A</li>
<li>5000个样例有5000个label，针对这些label，处理成为（01）格式,得到5000*10的矩阵B<ul>
<li>例如 y=5 处理为(0,0,0,0,1,0,0,0,0,0)</li>
</ul>
</li>
<li><code>costFunction = -log(A) .<em> B + (-log(1-A)) .</em> (1-B);</code>（注意：这里是点乘）</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">costFunction = -log(A) .* B + (-log(<span class="number">1</span>-A)) .* (<span class="number">1</span>-B);</span><br></pre></td></tr></table></figure>
<h3 id="反向传播">反向传播</h3><p>反向传播花了我好长时间，一直想用程序的角度来解决处理矩阵的问题，然后发现多此一举。</p>
<p>原来在反向传播中</p>
<ul>
<li>先做一遍正向的推导</li>
<li>从后向前计算误差值（关键点是：如果不是处理成向量来做，那么每一层的误差据真是不一样的）</li>
<li>计算梯度</li>
<li>正规化</li>
</ul>
<h2 id="我认为即使我做完了这些题目也没有什么好开心的，因为还是学不到最关键的部分，像是为什么反向传播可以求得costFunction_的导数。">我认为即使我做完了这些题目也没有什么好开心的，因为还是学不到最关键的部分，像是为什么反向传播可以求得costFunction 的导数。</h2></div><a href="/2015/11/01/机器学习5/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h1 class="post-title"><a href="/2015/10/31/机器学习4/" class="post-title-link">机器学习4</a></h1><div class="post-time">Oct 31, 2015</div><div class="post-content-excerpt"><h3 id="这一期的机器学习讲到了多类分类器和神经网络的铺垫">这一期的机器学习讲到了多类分类器和神经网络的铺垫</h3><p>在原来的两类分类中，我们只是用不是0就是1来分类，现在的n多类分类可以同理为不是x就是其他n-1种，最后对n中的特征的可能性取最大值即可。</p>
<p>公式可以参考两类分类中的公式，仅仅略有不同。</p>
<p>关于神经网络没有过多涉及，在下一讲中会有更多的体现吧。</p>
<p>仅仅说到了三层中，第一层是输入，第三层是输出，中间的层都是隐藏层，每一层由前一层推出，这叫做前向传导，据说后面还有后向传导。</p>
<p>没什么好说的，挺简单的。</p>
</div><a href="/2015/10/31/机器学习4/" class="read-more">...阅读全文</a></article></li></ul></section><footer><div class="paginator"><a href="/page/4/" class="prev">上一页</a><a href="/page/6/" class="next">下一页</a></div><div class="copyright"><p>© 2015 - 2016 <a href="http://fredchenjialin.github.io">Multipluso</a>, unless otherwise noted.</p></div></footer></div></body></html>